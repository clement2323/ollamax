% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ask_ollama.R
\name{ask_ollama}
\alias{ask_ollama}
\title{Poser une Question à Ollama en Utilisant un Modèle Spécifique}
\usage{
ask_ollama(
  question,
  model_name = "llama2",
  base_url = "https://ollama-clem.lab.sspcloud.fr"
)
}
\arguments{
\item{question}{\code{character} : La question ou le prompt à envoyer au modèle Ollama.}

\item{model_name}{\code{character} : Le nom du modèle à utiliser pour générer la réponse. Par défaut, \code{"llama2"}.}

\item{base_url}{\code{character} : L'URL de base de l'API Ollama. Par défaut, \code{"https://ollama-clem.lab.sspcloud.fr"}.}
}
\value{
\code{character} : La réponse générée par le modèle Ollama si la requête est réussie.
}
\description{
Cette fonction permet d'interroger l'API Ollama avec une question spécifique en utilisant un modèle donné.
Elle vérifie d'abord la disponibilité du modèle, le télécharge si nécessaire, puis envoie la requête
pour générer une réponse basée sur le modèle sélectionné.
}
\details{
La fonction effectue les opérations suivantes :
\itemize{
\item Vérifie si le modèle spécifié est disponible en utilisant \code{\link{check_model_availability}}.
\item Si le modèle n'est pas disponible, tente de le télécharger en utilisant \code{\link{pull_model}}.
\item Envoie une requête POST à l'API Ollama avec le prompt fourni.
\item Si la requête est réussie (code de statut HTTP 200), retourne la réponse générée par le modèle.
\item Si la requête échoue, arrête l'exécution avec un message d'erreur détaillé.
}
}
\examples{
\dontrun{
# Poser une question au modèle par défaut "llama2"
reponse <- ask_ollama("Quelle est la capitale de la France?")
print(reponse)

# Poser une question en spécifiant un autre modèle et une URL de base personnalisée
reponse <- ask_ollama("Explique la théorie de la relativité.", model_name = "gpt-4", base_url = "https://custom-api.example.com")
print(reponse)
}

}
