% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ask_ollama.R
\name{ask_ollama_vision}
\alias{ask_ollama_vision}
\title{Interroger le Modèle Llama3.2-Vision via Ollama}
\usage{
ask_ollama_vision(
  question,
  image_path = NULL,
  model_name = "llama3.2-vision",
  base_url = "https://ollama-clem.lab.sspcloud.fr",
  api_url = "/api/chat",
  structure_json =
    "{\\n    \\"model\\": \\"\%s\\",\\n    \\"messages\\": [\\n      {\\n        \\"role\\": \\"user\\",\\n        \\"content\\": \\"\%s\\",\\n        \\"images\\": [\\"\%s\\"]\\n      }\\n    ]\\n  }",
  process_response_function = process_ollama_response
)
}
\arguments{
\item{question}{\code{character} : La question ou instruction textuelle à envoyer au modèle.}

\item{image_path}{\code{character} : Chemin vers l'image à analyser (peut être NULL si pas d'image).}

\item{model_name}{\code{character} : Nom du modèle, par défaut \code{"llama3.2-vision"}.}

\item{base_url}{\code{character} : URL de base de l'API Ollama, par défaut \code{"https://ollama-clem.lab.sspcloud.fr"}.}
}
\value{
\code{character} : La réponse textuelle générée par le modèle.
}
\description{
Cette fonction permet d'interroger un modèle multimodal tel que \code{llama3.2-vision}
via l'API Ollama. En plus de la question textuelle, vous pouvez fournir un chemin
vers une image à analyser.
}
\details{
Cette fonction utilise l'endpoint \code{/api/chat} au lieu de \code{/api/generate},
car les modèles de vision attendent un format « chat » avec \code{images}.
}
